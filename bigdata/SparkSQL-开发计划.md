
# 基于SparkSQL的网站离线数据统计

## 1. 系统描述
> 随着网站业务量的增大，为解决传统关系数据库在大量数据统计分析上的性能以及负荷问题，需将数据计算部分搬离业务数据库，采用其它的
  具有分布式、伸缩性、海量数据处理能力的计算平台，鉴于业务数据统计都是基于现有存储在关系数据库中的数据，因此选择 SparkSQl 作为
  计算平台主要计算组件

## 2. 系统组件描述

### 2.1 数据存储
- 使用 Hadoop 作为分布式文件存储组件，使用其存储业务数据、日志数据，hive数据仓库
- 使用 Hive 作基于 Hadoop 的数据仓库，这里主要用作 SparkSQL 计算所需的数据基础
- 目前采用 cassandra 做中间结果存储组件

### 2.2 ETL
- 使用 Sqoop job 将业务数据增量导入到 Hive

### 2.3 计算组件
+ SparkCore Spark 计算通用组件，支持对多种数据格式的处理(转换、计算)
+ SparkSQL Spark 支持以关系数据库 sql 语法形式对数据进行分析计算，简化了一些相对简单的计算任务开发，加快开发进度。同时基于
  Spark 的内存模式的分布式弹性数据计算，其计算速度较 Hive 基于 Hadoop map-reduce 计算模型要快许多

## 3. SparkSQL 开发

### 3.1 Job 开发

+ 可使用 Spark 提供的 thriftserver driver 接口进行数据查询，其形式与其他 java RDMBS driver 无异
+ 可使用 hiveContext 直接执行请求执行的 SQL，根据是否异步考虑是否对客户端请求进行阻塞。若是异步执行，那么立即响应客户端，
  待结果计算完成后将将数据写入cassandra

> 比较：前者是 java RDBMS driver 操作方式，但对于数据存储没有后者方便(Spark-Cassandra提供了良好的API); 前者自定义函数相对麻烦，
  而后者使用方式时 hiveContext 直接提供了 udf(user defined function) 接口自定义函数;
  这两种方式可详细比较后斟酌选择


### 3.2 计算结果存储
  目前采用 spark-cassandra-driver 提供的数据库读写api将计算结果序列化为 json 格式保存到 cassandra 数据库


## 4. Job 管理

#### 4.1 Job 管理服务端
> 任务管理服务端使用 python 开发，使用组件有: tornado 作为web-framework，apsscheduler(类 unix crontab 格式任务调度框架)
  作为 spark 任务调度框架。对 UI 提供计算任务的管理api、计算结果查询api

####  实现功能
+ 任务查看:任务执行状态、任务执行耗时
+ 自定义 SparkSQL 执行计划，包括执行时间定义，执行计划SQL定义
+ 任务管理，包括立即执行任务、停止任务、删除任务


#### 4.2 任务管理前端
##### 实现功能
+ 支持任务查看、任务管理(立即执行任务、停止任务、删除任务)
+ 计算结果展示，主要根据任务ID(job_id)查询，可按时间范围筛选结果，支持按时间范围筛选结果(对于日统计，时间最大范围不超过30天)，
  不支持按页码筛选结果
+ 报表展示分简单表报和组合形式报表
	-  简单报表: 一个报表页面只展示一个计算任务统计的结果，只需根据 job_id 查询其结果即可
	-  组合结果: 一个页面展示多个计算任务的结果组合，需根据各个相关 job_id 一次或分多次查询数据


#### 目前进度
+ 基础组件(Hadoop,Hive,Spark,cassandra)运行过单节点及分布式环境
+ SparkSQL job 任务调用(执行，停止)
+ web-app 任务管理功能

#### 待完善
+ 更换 web-app web-framework from Flask to tornado
+ 代码重构优化

项目预计完成时间:  **2015-04-24**


#### TODO
+ 关系数据库导入 Hive 策略控制；如何应对关系数据库结构变化对 Hive schema 的影响
+ Hive 存储格式优化，分区、序列化等
+ 了解 spark 的资源管理，如何避免资源leak的同时又能增加并行任务数量
+ 环境初始化脚本规范

